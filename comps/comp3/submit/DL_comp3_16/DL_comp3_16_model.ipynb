{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bittempenvconda4fca1d7e168c4ef6b02d3a2f10225bde",
   "display_name": "Python 3.7.9 64-bit ('tempenv': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "import nets\n",
    "from utils import *\n",
    "from config import Config\n",
    "from dataset_m import prepare_dataset, prepare_dataset_cap, prepare_dataset_img, process_dataset\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "\n",
    "\tdef __init__(self, strategy, restore):\n",
    "\n",
    "\t\tself.strategy = strategy\n",
    "\t\tself.z_dim = Config.latent_dim\n",
    "\t\tself.global_batchsize = Config.global_batchsize\n",
    "\t\tself.batchsize_per_replica = int(self.global_batchsize/self.strategy.num_replicas_in_sync)\n",
    "\n",
    "\t\tself.gen_model = nets.Generator()\n",
    "\t\tself.disc_model = nets.Discriminator()\n",
    "\t\tself.gen_optimizer = tf.keras.optimizers.Adam(learning_rate=Config.gen_lr, beta_1=Config.beta1, beta_2=Config.beta2)\n",
    "\t\tself.disc_optimizer = tf.keras.optimizers.Adam(learning_rate=Config.disc_lr, beta_1=Config.beta1, beta_2=Config.beta2)\n",
    "\t\tself.train_writer = tf.summary.create_file_writer(Config.summaryDir+'train')\n",
    "\n",
    "\t\tself.ckpt = tf.train.Checkpoint(step=tf.Variable(0),\\\n",
    "\t\t\t\t\tgenerator_optimizer=self.gen_optimizer,\n",
    "\t\t\t\t\tgenerator_model = self.gen_model,\n",
    "\t\t\t\t\tdiscriminator_optimizer=self.disc_optimizer,\n",
    "\t\t\t\t\tdiscriminator_model=self.disc_model)\n",
    "\n",
    "\t\tself.ckpt_manager = tf.train.CheckpointManager(self.ckpt, Config.modelDir, max_to_keep=3)\n",
    "\n",
    "\t\tself.global_step = 0\n",
    "\n",
    "\t\tif(restore):\n",
    "\t\t\tlatest_ckpt= tf.train.latest_checkpoint(Config.modelDir)\n",
    "\t\t\tif not latest_ckpt:\n",
    "\t\t\t\traise Exception('No saved model found in: ' + Config.modelDir)\n",
    "\t\t\tself.ckpt.restore(latest_ckpt)\n",
    "\t\t\tself.global_step = int(latest_ckpt.split('-')[-1])   # .../ckpt-300 returns 300 previously trained totalbatches\n",
    "\t\t\tprint(\"Restored saved model from latest checkpoint\")\n",
    "\n",
    "\tdef compute_loss(self, labels, predictions):\n",
    "\n",
    "\t\tcross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True,\\\n",
    "\t\t\t\t\t\treduction=tf.keras.losses.Reduction.NONE)\n",
    "\t\treturn cross_entropy(labels, predictions)\n",
    "\n",
    "\tdef disc_loss(self, real_output, fake_output, wrong_output):\n",
    "\n",
    "\t\treal_loss = self.compute_loss(tf.ones_like(real_output), real_output)\n",
    "\t\tfake_loss = self.compute_loss(tf.zeros_like(fake_output), fake_output)\n",
    "\t\twrong_loss = self.compute_loss(tf.zeros_like(wrong_output), wrong_output)\n",
    "\t\ttotal_loss = real_loss + fake_loss + wrong_loss\n",
    "\t\ttotal_loss = total_loss/self.global_batchsize\n",
    "\t\treturn total_loss\n",
    "\n",
    "\tdef gen_loss(self, fake_output):\n",
    "\n",
    "\t\tgen_loss = self.compute_loss(tf.ones_like(fake_output), fake_output)\n",
    "\t\tgen_loss = gen_loss / self.global_batchsize\n",
    "\t\treturn gen_loss\n",
    "\n",
    "\t#@tf.function\n",
    "\tdef train_step(self, data):\n",
    "\t\t# img_c_ds, img_w_ds, cap_c_ds, cap_w_ds = data\n",
    "\t\treal_imgs, img_w_ds, cap_c_ds, cap_w_ds = data\n",
    "\t\t# real_imgs, img_w_ds = data\n",
    "\n",
    "\t\t# print(\"noise shape\", noise.shape)\n",
    "\t\t# print(\"caption shape\", cap_c_ds.shape)\n",
    "\t\t# print(f\"Cap_c_ds Shape: {cap_c_ds.shape}\")\n",
    "\n",
    "\t\twith tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\t\t\tgenerated_imgs = self.gen_model(cap_c_ds, training=True)\n",
    "\t\t\treal_output = self.disc_model(real_imgs, cap_c_ds, training=True)\n",
    "\t\t\twrong_output = self.disc_model(img_w_ds,cap_c_ds,training=True)\n",
    "\t\t\tfake_output = self.disc_model(generated_imgs, cap_c_ds, training=True)\n",
    "\t\t\td_loss = self.disc_loss(real_output, fake_output, wrong_output)\n",
    "\t\t\tg_loss = self.gen_loss(fake_output)\n",
    "\n",
    "\t\tG_grads = gen_tape.gradient(g_loss, self.gen_model.trainable_variables)\n",
    "\t\tD_grads = disc_tape.gradient(d_loss, self.disc_model.trainable_variables)\n",
    "\n",
    "\t\tself.gen_optimizer.apply_gradients(zip(G_grads, self.gen_model.trainable_variables))\n",
    "\t\tself.disc_optimizer.apply_gradients(zip(D_grads, self.disc_model.trainable_variables))\n",
    "\n",
    "\t\t#run g_optim twice to make sure d_loss doesn't go to zero\n",
    "\t\twith tf.GradientTape() as gen_tape:\n",
    "\t\t\tgenerated_imgs = self.gen_model(cap_c_ds, training=True)\n",
    "\t\t\tfake_output = self.disc_model(generated_imgs, cap_c_ds, training=True)\n",
    "\t\t\tg_loss = self.gen_loss(fake_output)\n",
    "\n",
    "\t\tG_grads = gen_tape.gradient(g_loss, self.gen_model.trainable_variables)\n",
    "\t\tself.gen_optimizer.apply_gradients(zip(G_grads, self.gen_model.trainable_variables))\n",
    "\n",
    "\t\treturn g_loss, d_loss\n",
    "\n",
    "\t#@tf.function\n",
    "\tdef gen_step(self, random_latents):\n",
    "\t\t# gen_imgs = []\n",
    "\t\t# for i in random_latents:\n",
    "\t\t# \tgen_img = self.gen_model(i, training=False)\n",
    "\t\t# \tgen_imgs.append(gen_img)\n",
    "\t\tgen_imgs = self.gen_model(random_latents, training=False)\n",
    "\t\treturn gen_imgs\n",
    "\n",
    "\t@tf.function\n",
    "\tdef distribute_trainstep(self, dist_dataset): # strategy.experimental_run_v2\n",
    "\t\tper_replica_g_losses, per_replica_d_losses = self.strategy.run(self.train_step,\\\n",
    "\t\t\t\t\t\t\t\t\t\t\targs=(dist_dataset,))\n",
    "\t\ttotal_g_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_g_losses,axis=0)\n",
    "\t\ttotal_d_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_d_losses, axis=0)\n",
    "\n",
    "\t\treturn total_g_loss, total_d_loss\n",
    "\n",
    "\t@tf.function\n",
    "\tdef distribute_genstep(self, dist_gen_noise):\n",
    "\n",
    "\t\tper_replica_genimgs = self.strategy.experimental_run_v2(self.gen_step, args=(dist_gen_noise,))\n",
    "\t\tgen_imgs = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_genimgs,axis=None)\n",
    "\t\treturn gen_imgs\n",
    "\n",
    "\t#@tf.function\n",
    "\tdef train_loop(self, num_epochs, dist_dataset, dist_noise_dataset):\n",
    "\n",
    "\t\tnum_batches = self.global_step\n",
    "\t\tfor i in range(num_epochs):\n",
    "\t\t\tprint('At Epoch {}'.format(i+1))\n",
    "\t\t\tprint('.........................................')\n",
    "\t\t\tfor one_batch in dist_dataset:\n",
    "\t\t\t\t# print(one_batch[0].shape)\n",
    "\t\t\t\t# print(one_batch[1].shape)\n",
    "\t\t\t\t# print(one_batch[2].shape)\n",
    "\t\t\t\t# print(one_batch[3].shape)\n",
    "\t\t\t\ttotal_g_loss, total_d_loss = self.distribute_trainstep(one_batch)\n",
    "\n",
    "\t\t\t\twith self.train_writer.as_default():\n",
    "\t\t\t\t\ttf.summary.scalar('generator_loss',total_g_loss, step=num_batches)\n",
    "\t\t\t\t\ttf.summary.scalar('discriminator_loss',total_d_loss, step=num_batches)\n",
    "\n",
    "\t\t\t\tif(num_batches % Config.image_snapshot_freq == 0):\n",
    "\t\t\t\t\tfor dist_gen_noise in dist_noise_dataset:\n",
    "\t\t\t\t\t\tgen_imgs = self.distribute_genstep(dist_gen_noise)\n",
    "\n",
    "\t\t\t\t\tfilename = Config.results_dir + 'fakes_epoch{:02d}_batch{:05d}.jpg'.format(i+1, num_batches)\n",
    "\t\t\t\t\tsave_image_grid(gen_imgs.numpy(), filename, drange=[-1,1], grid_size=Config.grid_size)\n",
    "\n",
    "\t\t\t\tnum_batches+=1\n",
    "\t\t\t\tprint('Gen_loss at batch {}: {:0.3f}'.format(num_batches, total_g_loss))\n",
    "\t\t\t\tprint('Disc_loss at batch {}: {:0.3f}'.format(num_batches, total_d_loss))\n",
    "\n",
    "\t\t\tself.ckpt.step.assign(i+1)\n",
    "\t\t\tself.ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(strategy, restore):\n",
    "\n",
    "\t# train_dataset = prepare_dataset(Config.tfrecord_dir+'train.tfrecord')\n",
    "\t# train_dataset = process_dataset(train_dataset)\n",
    "\t# dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "\t# Correspond image\n",
    "\timg_c_ds = prepare_dataset_img(Config.tfrecord_dir+'img_train.tfrecord')\n",
    "\t# Wrong image\n",
    "\timg_w_ds = prepare_dataset_img(Config.tfrecord_dir+'img_train.tfrecord')\n",
    "\t# Correspond text\n",
    "\tcap_c_ds = prepare_dataset_cap(Config.tfrecord_dir+'cap_train.tfrecord')\n",
    "\t# Wrong text\n",
    "\tcap_w_ds = prepare_dataset_cap(Config.tfrecord_dir+'cap_train.tfrecord')\n",
    "\n",
    "\tds = tf.data.Dataset.zip((img_c_ds, img_w_ds, cap_c_ds, cap_w_ds))\n",
    "\t# ds = tf.data.Dataset.zip((img_c_ds, img_w_ds))\n",
    "\tds = process_dataset(ds)\n",
    "\tdist_ds = strategy.experimental_distribute_dataset(ds)\n",
    "\t\n",
    "\n",
    "\t#periodic generation of fake images\n",
    "\tgen_noise = tf.random.normal(shape=[Config.num_gen_imgs, Config.latent_dim], seed=Config.random_seed)\n",
    "\tgen_dataset = tf.data.Dataset.from_tensor_slices(gen_noise).repeat(Config.num_gpu).batch(Config.num_gen_imgs*Config.num_gpu)\n",
    "\tdist_noise_dataset = strategy.experimental_distribute_dataset(gen_dataset)\n",
    "\n",
    "\twith strategy.scope():\n",
    "\n",
    "\t\tdcgan = DCGAN(strategy, restore)\n",
    "\t\t#num_epochs = tf.constant(Config.num_epochs, dtype=tf.int32)\n",
    "\t\tdcgan.train_loop(Config.num_epochs, dist_ds, dist_noise_dataset)\n",
    "\n",
    "\t# make_training_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(strategy, restore):\n",
    "\n",
    "\tgen_noise = tf.random.normal(shape=[Config.num_gen_imgs, Config.latent_dim])\n",
    "\tgen_dataset = tf.data.Dataset.from_tensor_slices(gen_noise).batch(Config.num_gen_imgs)\n",
    "\tdist_noise_dataset = strategy.experimental_distribute_dataset(gen_dataset)\n",
    "\twith strategy.scope():\n",
    "\n",
    "\t\tdcgan = DCGAN(strategy, restore)\n",
    "\t\tfor dist_gen_noise in dist_noise_dataset:\n",
    "\t\t\t# print(f\"Noise: {dist_gen_noise.shape}\")\n",
    "\t\t\tper_replica_genimgs = dcgan.distribute_genstep(dist_gen_noise)\n",
    "\n",
    "\tfilename = Config.results_dir + 'randomFakeGrid'\n",
    "\tsave_image_grid(per_replica_genimgs.numpy(), filename, drange=[-1,1], grid_size=Config.grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = ['/device:GPU:{}'.format(2)]\n",
    "strategy=tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "train(strategy, restore=False)"
   ]
  }
 ]
}