{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "<center>Shan-Hung Wu & DataLab</center>\n",
    "<center>Fall 2019</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lab, we use the tabular method (Q-learning, SARSA) to train an agent to play *Flappy Bird* with features in environments. However, it is time-costly and inefficient if more features are added to the environment because the agent can not easily generalize its experience to other states that were not seen before. Furthermore, in realistic environments with large state/action space, it requires a large memory space to store all state-action pairs.  \n",
    "In this lab, we introduce deep reinforcement learning, which utilizes function approximation to estimate value/policy for all unseen states such that given a state, we can estimate its value or action. We can use what we have learned in machine learning (e.g. regression, DNN) to achieve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep *Q*-Network\n",
    "*Reference*: [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)  \n",
    "To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations.  \n",
    "In this lab, we are going to train an agent which takes raw frames as input instead of hand-crafted features. The network architecture is as follows:\n",
    "<img src=\"./src/DQN-model-architecture.png\" alt=\"DQN-Architecture\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, you need to install pip in the environment and use pip under the environment to install package\n",
    "# conda install -c anaconda pip\n",
    "\n",
    "# Install PLE\n",
    "# !/home/ccchen/anaconda3/envs/tf2/bin/pip install git+git://github.com/ntasfi/PyGame-Learning-Environment\n",
    "\n",
    "# However, there is a bug that PLE miss folder assets, you need to clone the repo and copy folder assets to the environment\n",
    "# !git clone https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
    "# !cp -r ./PyGame-Learning-Environment/ple/games/flappybird/assets /home/ccchen/anaconda3/envs/tf2/lib/python3.6/site-packages/ple/games/flappybird/\n",
    "\n",
    "# Install other packages scikit-image, pygame, moviepy\n",
    "# !conda install -c anaconda scikit-image\n",
    "# !conda install -c cogsci pygame\n",
    "# !conda install -c conda-forge moviepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from skimage import data, color\n",
    "# from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "# image = color.rgb2gray(data.astronaut())\n",
    "\n",
    "# image_rescaled = rescale(image, 0.25, anti_aliasing=False)\n",
    "# image_resized = resize(image, (image.shape[0] // 4, image.shape[1] // 4),\n",
    "#                        anti_aliasing=True)\n",
    "# image_downscaled = downscale_local_mean(image, (4, 3))\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "# ax = axes.ravel()\n",
    "\n",
    "# ax[0].imshow(image, cmap='gray')\n",
    "# ax[0].set_title(\"Original image\")\n",
    "\n",
    "# ax[1].imshow(image_rescaled, cmap='gray')\n",
    "# ax[1].set_title(\"Rescaled image (aliasing)\")\n",
    "\n",
    "# ax[2].imshow(image_resized, cmap='gray')\n",
    "# ax[2].set_title(\"Resized image (no aliasing)\")\n",
    "\n",
    "# ax[3].imshow(image_downscaled, cmap='gray')\n",
    "# ax[3].set_title(\"Downscaled image (no aliasing)\")\n",
    "\n",
    "# ax[0].set_xlim(0, 512)\n",
    "# ax[0].set_ylim(512, 0)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[2], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.6.8)\nHello from the pygame community. https://www.pygame.org/contribute.html\ncouldn't import doomish\nCouldn't import doom\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ple import PLE\n",
    "from ple.games.flappybird import FlappyBird\n",
    "# from ple.games.flappybird import FlappyBird\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line make pop-out window not appear\n",
    "\n",
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
    "env.reset_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Estimation\n",
    "Remind that we can use TD-estimation to update the Q-value either using *Q*-learning or SARSA. The basic idea of *Q*-learning is to approximate the Q-value by neural networks in the fashion of *Q*-learning. We can formalize the algorithm as follows:\n",
    "- Use a DNN $f_{Q^*}(s,a;\\theta)$ to represent $Q^*(s,a)$.\n",
    "    <img src=\"./src/function-approximator.PNG\" alt=\"function-approximator\" width=\"180\"/>\n",
    "- Algorithm(TD): initialize $\\theta$ arbitraily, iterate until converge:\n",
    "    1. Take action $a$ from $s$ using some exploration policy $\\pi'$ derived from $f_{Q^*}$ (e.g., $\\epsilon$-greedy).\n",
    "    2. Observe $s'$ and reward $R(s,a,s')$, update $\\theta$ using SGD:\n",
    "        $$\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}C,\\text{where}$$\n",
    "        $$C(\\theta)=[\\color{blue}{R(s,a,s')+\\gamma\\max_{a'}f_{Q^*}(s',a';\\theta)}-f_{Q^*}(s,a;\\theta)]^2$$\n",
    "\n",
    "However, DQN based on the naive TD algorithm above diverges due to:  \n",
    "1. Samples are correlated (violates i.i.d. assumption of training examples).\n",
    "2. Non-stationary target ($\\color{blue}{f_{Q^*}(s',a';\\theta)}$ changes as $\\theta$ is updated for current $a$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilization Techniques\n",
    "- Experience replay: To break the correlations present in the sequence of observations.\n",
    "    1. Use a replay memory $D$ to store recently seen transitions $(s,a,r,s')$.\n",
    "    2. Sample a mini-batch from $D$ and update $\\theta$. \n",
    "- Delayed target network: To avoid chasing a moving target.\n",
    "    1. Set the target value to the output of the network parameterized by *old* $\\theta^-$.\n",
    "    2. Update $\\theta^-\\leftarrow\\theta$ every $K$ iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "Combining Algorithm(TD) with Experience replay and Delayed target network, we can formalize the complete DQN algorithm as below:  \n",
    "- Algorithm(TD): initialize $\\theta$ arbitraily and $\\theta^-=\\theta$, iterate until converge:\n",
    "    1. Take action $a$ from $s$ using some exploration policy $\\pi'$ derived from $f_{Q^*}$ (e.g., $\\epsilon$-greedy).\n",
    "    2. Observe $s'$ and reward $R(s,a,s')$, add $(s,a,R,s')$ to $D$.\n",
    "    3. Sample a mini-batch of $(s,a,R,s^{'})^,\\text{s}$ from $D$, do:\n",
    "        $$\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}C,\\text{where}$$\n",
    "        $$C(\\theta)=[\\color{blue}{R(s,a,s')+\\gamma\\max_{a'}f_{Q^*}(s',a';\\color{red}{\\theta^-})}-f_{Q^*}(s,a;\\theta)]^2$$\n",
    "    4. Update $\\theta^-\\leftarrow\\theta$ every $K$ iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement DQN and apply it on Flappy Bird now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input Size\n",
    "IMG_WIDTH = 84\n",
    "IMG_HEIGHT = 84\n",
    "NUM_STACK = 4\n",
    "# Modify\n",
    "NUM_STATE_FEATURE = 8\n",
    "# For Epsilon-greedy\n",
    "MIN_EXPLORING_RATE = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name, num_action, discount_factor=0.99):\n",
    "        self.exploring_rate = 0.1\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_action = num_action\n",
    "        self.model = self.build_model(name)\n",
    "\n",
    "    def build_model(self, name):\n",
    "        # input: state\n",
    "        # output: each action's Q-value \n",
    "#         screen_stack = tf.keras.Input(shape=[IMG_WIDTH, IMG_HEIGHT, NUM_STACK], dtype=tf.float32)\n",
    "        # Modify\n",
    "        input_data = tf.keras.Input(shape=[NUM_STATE_FEATURE], dtype=tf.float32)\n",
    "\n",
    "#         x = tf.keras.layers.Conv2D(filters=32, kernel_size=8, strides=4)(screen_stack)\n",
    "#         x = tf.keras.layers.ReLU()(x)\n",
    "#         x = tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=2)(x)\n",
    "#         x = tf.keras.layers.ReLU()(x)\n",
    "#         x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)(x)\n",
    "#         x = tf.keras.layers.ReLU()(x)\n",
    "#         x = tf.keras.layers.Flatten()(x)\n",
    "        # Modify\n",
    "        x = tf.keras.layers.Dense(units=512)(input_data)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = tf.keras.layers.Dense(units=512)(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = tf.keras.layers.Dense(units=512)(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        Q = tf.keras.layers.Dense(self.num_action)(x)\n",
    "\n",
    "        model = tf.keras.Model(name=name, inputs=input_data, outputs=Q)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def loss(self, state, action, reward, tar_Q, ternimal):\n",
    "        # Q(s,a,theta) for all a, shape (batch_size, num_action)\n",
    "        output = self.model(state)\n",
    "        index = tf.stack([tf.range(tf.shape(action)[0]), action], axis=1)\n",
    "        # Q(s,a,theta) for selected a, shape (batch_size, 1)\n",
    "        Q = tf.gather_nd(output, index)\n",
    "        \n",
    "        # set tar_Q as 0 if reaching terminal state\n",
    "        tar_Q *= ~np.array(terminal)\n",
    "\n",
    "        # loss = E[r+max(Q(s',a',theta'))-Q(s,a,theta)]\n",
    "        loss = tf.reduce_mean(tf.square(reward + self.discount_factor * tar_Q - Q))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def max_Q(self, state):\n",
    "        # Q(s,a,theta) for all a, shape (batch_size, num_action)\n",
    "        output = self.model(state)\n",
    "\n",
    "        # max(Q(s',a',theta')), shape (batch_size, 1)\n",
    "        return tf.reduce_max(output, axis=1)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < self.exploring_rate:\n",
    "            action = np.random.choice(self.num_action)  # Select a random action\n",
    "        else:\n",
    "            state = np.expand_dims(state, axis = 0)\n",
    "            # Q(s,a,theta) for all a, shape (batch_size, num_action)\n",
    "            output = self.model(state)\n",
    "\n",
    "            # select action with highest action-value\n",
    "            action = tf.argmax(output, axis=1)[0]\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update_parameters(self, episode):\n",
    "        self.exploring_rate = max(MIN_EXPLORING_RATE, min(0.5, 0.99**((episode) / 30)))\n",
    "\n",
    "    def shutdown_explore(self):\n",
    "        # make action selection greedy\n",
    "        self.exploring_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init agent\n",
    "num_action = len(env.getActionSet())\n",
    "\n",
    "# agent for frequently updating\n",
    "online_agent = Agent('online', num_action)\n",
    "\n",
    "# agent for slow updating\n",
    "target_agent = Agent('target', num_action)\n",
    "# synchronize target model's weight with online model's weight\n",
    "target_agent.model.set_weights(online_agent.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "average_loss = tf.keras.metrics.Mean(name='loss')\n",
    "\n",
    "@tf.function\n",
    "def train_step(state, action, reward, next_state, ternimal):\n",
    "    # Delayed Target Network\n",
    "    tar_Q = target_agent.max_Q(next_state)\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = online_agent.loss(state, action, reward, tar_Q, ternimal)\n",
    "    gradients = tape.gradient(loss, online_agent.model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, online_agent.model.trainable_variables))\n",
    "    \n",
    "    average_loss.update_state(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.experiences = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.experiences) >= self.buffer_size:\n",
    "            self.experiences.pop(0)\n",
    "        self.experiences.append(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        \"\"\"\n",
    "        sample experience from buffer\n",
    "        \"\"\"\n",
    "        if size > len(self.experiences):\n",
    "            experiences_idx = np.random.choice(len(self.experiences), size=size)\n",
    "        else:\n",
    "            experiences_idx = np.random.choice(len(self.experiences), size=size, replace=False)\n",
    "\n",
    "        # from all sampled experiences, extract a tuple of (s,a,r,s')\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_prime = []\n",
    "        terminal = []\n",
    "        for i in range(size):\n",
    "            states.append(self.experiences[experiences_idx[i]][0])\n",
    "            actions.append(self.experiences[experiences_idx[i]][1])\n",
    "            rewards.append(self.experiences[experiences_idx[i]][2])\n",
    "            states_prime.append(self.experiences[experiences_idx[i]][3])\n",
    "            terminal.append(self.experiences[experiences_idx[i]][4])\n",
    "\n",
    "        return states, actions, rewards, states_prime, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init buffer\n",
    "buffer = Replay_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1228 02:33:35.492488 140305104299840 core.py:204] In /home/ccchen/anaconda3/envs/tf2/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "W1228 02:33:35.493338 140305104299840 core.py:204] In /home/ccchen/anaconda3/envs/tf2/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "W1228 02:33:35.493818 140305104299840 core.py:204] In /home/ccchen/anaconda3/envs/tf2/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "W1228 02:33:35.494578 140305104299840 core.py:204] In /home/ccchen/anaconda3/envs/tf2/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "W1228 02:33:35.494971 140305104299840 core.py:204] In /home/ccchen/anaconda3/envs/tf2/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "W1228 02:33:35.495347 140305104299840 core.py:204] In /home/ccchen/anaconda3/envs/tf2/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "W1228 02:33:35.496175 140305104299840 core.py:204] In /home/ccchen/anaconda3/envs/tf2/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "W1228 02:33:35.496578 140305104299840 core.py:204] In /home/ccchen/anaconda3/envs/tf2/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mpy\n",
    "\n",
    "def make_anim(images, fps=60, true_image=False):\n",
    "    duration = len(images) / fps\n",
    "\n",
    "    def make_frame(t):\n",
    "        try:\n",
    "            x = images[int(len(images) / duration * t)]\n",
    "        except:\n",
    "            x = images[-1]\n",
    "\n",
    "        if true_image:\n",
    "            return x.astype(np.uint8)\n",
    "        else:\n",
    "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "    clip.fps = fps\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import skimage.transform\n",
    "\n",
    "# def preprocess_screen(screen):\n",
    "#     screen = skimage.transform.resize(screen, [IMG_WIDTH, IMG_HEIGHT, 1])\n",
    "#     return screen\n",
    "\n",
    "# def frames_to_state(input_frames):\n",
    "#     if(len(input_frames) == 1):\n",
    "#         state = np.concatenate(input_frames*4, axis=-1)\n",
    "#     elif(len(input_frames) == 2):\n",
    "#         state = np.concatenate(input_frames[0:1]*2 + input_frames[1:]*2, axis=-1)\n",
    "#     elif(len(input_frames) == 3):\n",
    "#         state = np.concatenate(input_frames + input_frames[2:], axis=-1)\n",
    "#     else:\n",
    "#         state = np.concatenate(input_frames[-4:], axis=-1)\n",
    "\n",
    "#     return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state(env_state):\n",
    "    state = np.zeros(NUM_STATE_FEATURE)\n",
    "    state[0] = env_state['player_y']\n",
    "    state[1] = env_state['player_vel']\n",
    "    state[2] = env_state['next_pipe_dist_to_player']\n",
    "    state[3] = env_state['next_pipe_top_y'] - env_state['player_y']\n",
    "    state[4] = env_state['next_pipe_bottom_y'] - env_state['player_y']\n",
    "    state[5] = env_state['next_next_pipe_dist_to_player']\n",
    "    state[6] = env_state['next_next_pipe_top_y'] - env_state['player_y']\n",
    "    state[7] = env_state['next_next_pipe_bottom_y'] - env_state['player_y']\n",
    "    \n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "t:   3%|▎         | 2/63 [01:01<31:18, 30.79s/it, now=None]\n",
      "\n",
      "t:   3%|▎         | 2/63 [01:01<31:19, 30.81s/it, now=None]\n",
      "t:   3%|▎         | 2/63 [00:27<13:58, 13.75s/it, now=None]\u001b[AMoviepy - Building video sychou/lab17/movie_f/DQN_demo-0.mp4.\n",
      "Moviepy - Writing video sychou/lab17/movie_f/DQN_demo-0.mp4\n",
      "\n",
      "\n",
      "\n",
      "t:   0%|          | 0/63 [00:00<?, ?it/s, now=None]\u001b[A\u001b[A"
     ]
    },
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "[Errno 32] Broken pipe\n\nMoviePy error: FFMPEG encountered the following error while writing file sychou/lab17/movie_f/DQN_demo-0.mp4:\n\n b'sychou/lab17/movie_f/DQN_demo-0.mp4: No such file or directory\\n'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mwrite_frame\u001b[0;34m(self, img_array)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-d7e253e1f422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_video_every_episode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for every 500 episode, record an animation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_anim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sychou/lab17/movie_f/DQN_demo-{}.mp4\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-180>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-179>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    133\u001b[0m              for (k,v) in k.items()}\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-178>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    305\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                            \u001b[0mffmpeg_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mffmpeg_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                            logger=logger)\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_logfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mwrite_frame\u001b[0;34m(self, img_array)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 32] Broken pipe\n\nMoviePy error: FFMPEG encountered the following error while writing file sychou/lab17/movie_f/DQN_demo-0.mp4:\n\n b'sychou/lab17/movie_f/DQN_demo-0.mp4: No such file or directory\\n'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "update_every_iteration = 1000\n",
    "print_every_episode = 500\n",
    "save_video_every_episode = 5000\n",
    "NUM_EPISODE = 20001\n",
    "NUM_EXPLORE = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "iter_num = 0\n",
    "for episode in range(0, NUM_EPISODE + 1):\n",
    "    \n",
    "    # Reset the environment\n",
    "    env.reset_game()\n",
    "    \n",
    "    # record frame\n",
    "    if episode % save_video_every_episode == 0:\n",
    "        frames = [env.getScreenRGB()]\n",
    "    \n",
    "    # input frame\n",
    "#     input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
    "    \n",
    "    # for every 500 episodes, shutdown exploration to see the performance of greedy action\n",
    "    if episode % print_every_episode == 0:\n",
    "        online_agent.shutdown_explore()\n",
    "    \n",
    "    # cumulate reward for this episode\n",
    "    cum_reward = 0\n",
    "    \n",
    "    t = 0\n",
    "    while not env.game_over():\n",
    "        \n",
    "#         state = frames_to_state(input_frames)\n",
    "        # Modify\n",
    "        state = make_state(game.getGameState())\n",
    "        \n",
    "        # feed current state and select an action\n",
    "        action = online_agent.select_action(state)\n",
    "        \n",
    "        # execute the action and get reward\n",
    "        reward = env.act(env.getActionSet()[action])\n",
    "        \n",
    "        # record frame\n",
    "        if episode % save_video_every_episode == 0:\n",
    "            frames.append(env.getScreenRGB())\n",
    "        \n",
    "        # record input frame\n",
    "#         input_frames.append(preprocess_screen(env.getScreenGrayscale()))\n",
    "        \n",
    "        # cumulate reward\n",
    "        cum_reward += reward\n",
    "        \n",
    "        # observe the result\n",
    "#         state_prime = frames_to_state(input_frames)  # get next state\n",
    "        # Modify\n",
    "        state_prime = make_state(game.getGameState())\n",
    "        \n",
    "        # append experience for this episode\n",
    "        if episode % print_every_episode != 0:\n",
    "            buffer.add((state, action, reward, state_prime, env.game_over()))\n",
    "        \n",
    "        # Setting up for the next iteration\n",
    "        state = state_prime\n",
    "        t += 1\n",
    "        \n",
    "        # update agent\n",
    "        if episode > NUM_EXPLORE and episode % print_every_episode != 0:\n",
    "            iter_num += 1\n",
    "            train_states, train_actions, train_rewards, train_states_prime, terminal = buffer.sample(BATCH_SIZE)\n",
    "#             train_states = np.asarray(train_states).reshape(-1, IMG_WIDTH, IMG_HEIGHT, NUM_STACK)\n",
    "#             train_states_prime = np.asarray(train_states_prime).reshape(-1, IMG_WIDTH, IMG_HEIGHT, NUM_STACK)\n",
    "            # Modify\n",
    "#             print(train_states)\n",
    "            train_states = np.asarray(train_states)\n",
    "            train_states_prime = np.asarray(train_states_prime)\n",
    "            \n",
    "            # convert Python object to Tensor to prevent graph re-tracing\n",
    "            train_states = tf.convert_to_tensor(train_states, tf.float32)\n",
    "            train_actions = tf.convert_to_tensor(train_actions, tf.int32)\n",
    "            train_rewards = tf.convert_to_tensor(train_rewards, tf.float32)\n",
    "            train_states_prime = tf.convert_to_tensor(train_states_prime, tf.float32)\n",
    "            terminal = tf.convert_to_tensor(terminal, tf.bool)\n",
    "            \n",
    "            train_step(train_states, train_actions, train_rewards, train_states_prime, terminal)\n",
    "\n",
    "        # synchronize target model's weight with online model's weight every 1000 iterations\n",
    "        if iter_num % update_every_iteration == 0 and episode > NUM_EXPLORE and episode % print_every_episode != 0:\n",
    "            target_agent.model.set_weights(online_agent.model.get_weights())\n",
    "\n",
    "    # update exploring rate\n",
    "    online_agent.update_parameters(episode)\n",
    "    target_agent.update_parameters(episode)\n",
    "\n",
    "    if episode % print_every_episode == 0 and episode > NUM_EXPLORE:\n",
    "        print(\n",
    "            \"[{}] time live:{}, cumulated reward: {}, exploring rate: {}, average loss: {}\".\n",
    "            format(episode, t, cum_reward, online_agent.exploring_rate, average_loss.result()))\n",
    "        average_loss.reset_states()\n",
    "\n",
    "    if episode % save_video_every_episode == 0:  # for every 500 episode, record an animation\n",
    "        clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
    "        clip.write_videofile(\"sychou/labs/lab17/movie_f/DQN_demo-{}.mp4\".format(episode), fps=60)\n",
    "        display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "print(\"DEMO Result\")\n",
    "clip = VideoFileClip(\"movie_f/DQN_demo-20000.mp4\")\n",
    "display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}